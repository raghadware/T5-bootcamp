{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arabic Hate Speech - Detecting Offensive Language\n",
        "\n",
        "\n",
        "#### Fine-Grained Hate Speech Detection on Arabic Twitter\n",
        "**Disclaimer: Some examples have offensive language and hate speech!**\n",
        "\n",
        "\n",
        "Detecting offensive language and hate speech is very important for online safety, content moderation, etc. Studies show that the presence of hate speech may be connected to hate crimes (Hate Speech Watch, 2014).\n",
        "\n",
        "Given the largest annotated Arabic tweets without being biased towards specific topics, genres, or dialects. Each tweet is judged by 3 annotators using crowdsourcing for offensiveness. Offensive tweets were classified into one of the hate speech types: Race, Religion, Ideology, Disability, Social Class, and Gender. Also, annotators judged whether a tweet has vulgar language or violence.\n",
        "\n",
        "Hate speech is defined as any kind of offensive language (insults, slurs, threats, encouraging violence, impolite language, etc.) that targets a person or a group of people based on common characteristics such as race/ethnicity/nationality, religion/belief, ideology, disability/disease, social class, gender, etc.\n",
        "\n",
        "\n",
        "Hate Speech types in our dataset are:\n",
        "HS1 (race/ethnicity/nationality).\n",
        "HS2 (religion/belief).\n",
        "HS3 (ideology).\n",
        "HS4 (disability/disease).\n",
        "HS5 (social class).\n",
        "HS6 (gender).\n",
        "\n",
        "\n",
        "The corpus contains ~13K tweets in total: 35% are offensive and 11% are hate speech. Vulgar and violent tweets represent 1.5% and 0.7% of the whole corpus.\n",
        "\n",
        "\n",
        "#### This task consists of 3 subtasks:\n",
        "\n",
        "**Subtask A:** Detect whether a tweet is offensive or not.\n",
        "Labels for this task are: OFF (Offensive) or NOT_OFF (Not Offensive)\n",
        "Example: الله يلعنه على هالسؤال (May God curse him for this question! )\n",
        "\n",
        "**Subtask B:** Detect whether a tweet has hate speech or not.\n",
        "Labels are: HS (Hate Speech) or NOT_HS (Not Hate Speech).\n",
        "Subtask B is more challenging than Subtask A as 11% only of the tweets are labeled as hate speech.\n",
        "Example: أنتم شعب متخلف (You are a retarded people)\n",
        "\n",
        "\n",
        "**Subtask C:** Detect the fine-grained type of hate speech.\n",
        "Labels are: HS1 (Race), HS2 (Religion), HS3 (Ideology), HS4 (Disability), HS5 (Social Class), and HS6 (Gender).\n",
        "A tweet takes only one label for hate speech type based on the majority voting of the 3 annotators. In case there is no majority label, the final label was determined by a domain expert.\n",
        "\n",
        "\n",
        "Data has been split into 70% for training, 10% for development, and 20% for testing."
      ],
      "metadata": {
        "id": "t2W9BhMw7lol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">### **Dataset**\n",
        ">Download training data from: https://alt.qcri.org/resources/OSACT2022/>OSACT2022-sharedTask-train.txt\n",
        ">\n",
        ">Download development data from: https://alt.qcri.org/resources/OSACT2022/>OSACT2022-sharedTask-dev.txt\n",
        ">\n",
        ">Download test data from: https://alt.qcri.org/resources/OSACT2022/>OSACT2022-sharedTask-test-tweets.txt"
      ],
      "metadata": {
        "id": "xaLeoJ5r9ptA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Necessary Imports"
      ],
      "metadata": {
        "id": "GYhCT7TIlgDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3i_JOOoWIA-G",
        "outputId": "6599f1a4-5f79-47da-dc11-997569baac74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install pyarabic\n",
        "!pip install tkseem\n",
        "#!pip install Data_Fetching\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByLDGpt1hyLM",
        "outputId": "13b85ad5-6ab2-4e59-cb35-58ae41ac1c38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/397.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/397.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n",
            "Collecting tkseem\n",
            "  Downloading tkseem-0.0.3-py3-none-any.whl (30.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.9/30.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from tkseem)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting farasapy (from tkseem)\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tkseem) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tkseem) (1.23.5)\n",
            "Collecting black (from tkseem)\n",
            "  Downloading black-23.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->tkseem) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->tkseem)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->tkseem) (23.2)\n",
            "Collecting pathspec>=0.9.0 (from black->tkseem)\n",
            "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->tkseem) (4.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->tkseem) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->tkseem) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->tkseem) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->tkseem) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->tkseem) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->tkseem) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->tkseem) (2023.11.17)\n",
            "Installing collected packages: sentencepiece, pathspec, mypy-extensions, farasapy, black, tkseem\n",
            "Successfully installed black-23.11.0 farasapy-0.0.14 mypy-extensions-1.0.0 pathspec-0.11.2 sentencepiece-0.1.99 tkseem-0.0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from gensim.utils import simple_preprocess\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "from pyarabic.araby import strip_tashkeel\n",
        "from pyarabic.araby import normalize_ligature\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from gensim.models import FastText\n",
        "import tkseem as tk"
      ],
      "metadata": {
        "id": "ZFGxWmNdlicW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Subtask A**"
      ],
      "metadata": {
        "id": "8I0WrEJJ9DWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "vGuVyiXeV2yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOP_WORDS = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "\n",
        "def tokenize_tweet(tweet):\n",
        "    # create a TweetTokenizer object\n",
        "    tknzr = TweetTokenizer()\n",
        "    # tokenize the tweet\n",
        "    tokens = tknzr.tokenize(tweet)\n",
        "    return tokens\n",
        "\n",
        "def remove_extra_spaces(words):\n",
        "    \"\"\"Removes extra whitespaces at the beginning and at the end of each word in a list\"\"\"\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        cleaned_word = ' '.join(word.split()).strip()\n",
        "        cleaned_words.append(cleaned_word)\n",
        "    return cleaned_words\n",
        "\n",
        "def remove_urls(lst):\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return [re.sub(pattern, '', item).strip() for item in lst if re.sub(pattern, '', item).strip() != '']\n",
        "\n",
        "def remove_user_mentions(words):\n",
        "    \"\"\"Removes user mentions (@user) from a list of words\"\"\"\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        if not word.startswith('@'):\n",
        "            cleaned_words.append(word)\n",
        "    return cleaned_words\n",
        "\n",
        "def remove_punctuation(lst):\n",
        "    \"\"\"Removes punctuation from a list of strings, including single punctuation characters\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation+'؟')\n",
        "    result = []\n",
        "    for item in lst:\n",
        "        # Remove all punctuation characters\n",
        "        item = item.translate(translator)\n",
        "        # Remove any remaining single punctuation characters\n",
        "        if item != '':\n",
        "          result.append(item)\n",
        "    return result\n",
        "\n",
        "def remove_numbers(lst):\n",
        "    \"\"\"Removes numbers from a list of strings\"\"\"\n",
        "    pattern = re.compile(r'\\d+')\n",
        "    return [re.sub(pattern, '', item) for item in lst if re.sub(pattern, '', item).strip() != '']\n",
        "\n",
        "def remove_emojis(words):\n",
        "    \"\"\"Removes emojis from a list of words\"\"\"\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        cleaned_word = ''.join(c for c in word if c not in emoji.EMOJI_DATA)\n",
        "        if cleaned_word != '':\n",
        "            cleaned_words.append(cleaned_word)\n",
        "    return cleaned_words\n",
        "\n",
        "def remove_foreign_language(lst):\n",
        "    pattern = re.compile(r'[^\\u0600-\\u06ff]+')\n",
        "    return [re.sub(pattern, \"\", item) for item in lst if re.sub(pattern, \"\", item) != '']\n",
        "\n",
        "def remove_tashkeel(lst):\n",
        "    return [normalize_ligature(strip_tashkeel(word)) for word in lst]\n",
        "\n",
        "def remove_repeated_chars(lst):\n",
        "    pattern = re.compile(r\"(\\w)\\1{2,}\")\n",
        "    return [re.sub(pattern, r\"\\1\\1\", item).strip() for item in lst if re.sub(pattern, '', item).strip() != '']\n",
        "\n",
        "def remove_stop_words(lst):\n",
        "\n",
        "    result = []\n",
        "    for word in lst:\n",
        "        if word not in STOP_WORDS:\n",
        "            result.append(word)\n",
        "    return result\n",
        "\n",
        "def form_sentence(words):\n",
        "    \"\"\"Forms a sentence from a list of words\"\"\"\n",
        "    sentence = ' '.join(words)\n",
        "    return sentence\n",
        "\n",
        "def clean_tweet(tweet,mode=\"ml\"):\n",
        "    \"\"\"\n",
        "    A function to clean a single tweet.\n",
        "    \"\"\"\n",
        "    if mode==\"ml\":\n",
        "        #tokenize tweet\n",
        "        words = tokenize_tweet(tweet)\n",
        "        #remove extra white-spaces\n",
        "        words = remove_extra_spaces(words)\n",
        "        #remove urls\n",
        "        words = remove_urls(words)\n",
        "        #remove user mentions\n",
        "        words = remove_user_mentions(words)\n",
        "        #remove punctiation\n",
        "        words = remove_punctuation(words)\n",
        "        #remove numbers\n",
        "        words = remove_numbers(words)\n",
        "        #remove emojis\n",
        "        words = remove_emojis(words)\n",
        "        #remove non-arabic charachters\n",
        "        words = remove_foreign_language(words)\n",
        "        #remove tashkeel\n",
        "        words = remove_tashkeel(words)\n",
        "        #remove repeated charachters\n",
        "        words = remove_repeated_chars(words)\n",
        "        #remove stop words\n",
        "        words = remove_stop_words(words)\n",
        "        #form a new sentence\n",
        "        sentence = form_sentence(words)\n",
        "    else:\n",
        "        words = tokenize_tweet(tweet)\n",
        "        #remove extra white-spaces\n",
        "        words = remove_extra_spaces(words)\n",
        "        #remove urls\n",
        "        words = remove_urls(words)\n",
        "        #remove user mentions\n",
        "        words = remove_user_mentions(words)\n",
        "        #remove punctiation\n",
        "        words = remove_punctuation(words)\n",
        "        #remove numbers\n",
        "        words = remove_numbers(words)\n",
        "        #remove emojis\n",
        "        words = remove_emojis(words)\n",
        "        #remove non-arabic charachters\n",
        "        words = remove_foreign_language(words)\n",
        "        #form a new sentence\n",
        "        sentence = form_sentence(words)\n",
        "    return sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74WXSjaoi8jo",
        "outputId": "ee1b4ff3-3c23-4f0a-b4b9-c6ee1dbdc94e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad_tweets(data,datatype= 'train',max_words=None,max_seq_len=None,model_path= './Models/tokenizer_model.pkl'):\n",
        "    \"\"\"Tokenizes tweets and pads the sequences to the length of the longest sequence in the dataset.\n",
        "\n",
        "    Args:\n",
        "        df_column (pandas.Series): A DataFrame column containing tweets.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            numpy.ndarray: An array of padded sequences.\n",
        "            int: The vocabulary size.\n",
        "            int: The maximum sequence length.\n",
        "            Tokenizer: The tokenizer object used for the tokenization.\n",
        "    \"\"\"\n",
        "    PROJECT_PATH = os.path.realpath(os.path.dirname(__file__))\n",
        "    model_path = os.path.join(PROJECT_PATH, model_path)\n",
        "\n",
        "    if max_words == None:\n",
        "          tokenizer = tk.WordTokenizer()\n",
        "    else:\n",
        "          tokenizer = tk.WordTokenizer(vocab_size=max_words)\n",
        "\n",
        "    if datatype == 'train':\n",
        "        # Create tokenizer\n",
        "        path = os.path.join(PROJECT_PATH, './Data/tokenizer.txt')\n",
        "        df = pd.DataFrame(data,columns=['tweet'])\n",
        "        df.to_csv(path, sep='\\n', header=False,index=False)\n",
        "\n",
        "        tokenizer.train(path)\n",
        "\n",
        "        sequences = [tokenizer.encode(sentence) for sentence in data]\n",
        "        max_seq_len = max(len(seq) for seq in sequences)\n",
        "\n",
        "        vocab_size = tokenizer.vocab_size\n",
        "        sequences = pad_sequences(sequences, maxlen=max_seq_len,value = 0, padding='post')\n",
        "\n",
        "\n",
        "        tokenizer.save_model(os.path.join(PROJECT_PATH,'./Models/tokenizer_model.pkl'))\n",
        "\n",
        "    elif datatype == 'test' and max_seq_len != None:\n",
        "        try:\n",
        "            tokenizer.load_model(model_path)\n",
        "            sequences = [tokenizer.encode(sentence) for sentence in data]\n",
        "            vocab_size = tokenizer.vocab_size\n",
        "            sequences = pad_sequences(sequences, maxlen=max_seq_len,value = 0, padding='post')\n",
        "\n",
        "        except:\n",
        "            print(\"please check if tokenizer model is passed correctly!\")\n",
        "\n",
        "    return sequences, vocab_size, max_seq_len, tokenizer\n"
      ],
      "metadata": {
        "id": "aG9ns3Wy82he"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Class"
      ],
      "metadata": {
        "id": "7176IL_YV6mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels_path = '/content/drive/MyDrive/hate-speech/Data/subtask_A_labels.txt'\n",
        "with open(test_labels_path, 'r', encoding='utf-8') as file:\n",
        "      test_labels = file.readlines()"
      ],
      "metadata": {
        "id": "PTPJpcu5fJgk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier:\n",
        "  def __init__(self, folder_path,label_name):\n",
        "    self.folder_path = folder_path\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test, self.X_dev, self.y_dev = None, None, None, None, None, None\n",
        "    self.model = None\n",
        "    self.label_name = label_name\n",
        "    self.max_seq_length  = None\n",
        "    self.vocab_size = None\n",
        "\n",
        "  def tokenize_and_pad_tweets(self, data, datatype= 'train', max_words=None, max_seq_len=None, model_path= '/content/tokenizer_model.pkl'):\n",
        "\n",
        "      sequences = [] # Initialize sequences to an empty list\n",
        "\n",
        "      if max_words == None:\n",
        "          tokenizer = tk.WordTokenizer()\n",
        "      else:\n",
        "          tokenizer = tk.WordTokenizer(vocab_size=max_words)\n",
        "          self.vocab_size = max_words\n",
        "\n",
        "      if datatype == 'train':\n",
        "          path = './tokenizer.txt'\n",
        "          df = pd.DataFrame(data,columns=['tweet'])\n",
        "          df.to_csv(path, sep='\\n', header=False,index=False)\n",
        "          tokenizer.train(path)\n",
        "          sequences = [tokenizer.encode(sentence) for sentence in data]\n",
        "          max_seq_len = max(len(seq) for seq in sequences)\n",
        "          sequences = pad_sequences(sequences, maxlen=max_seq_len, value = 0, padding='post')\n",
        "          tokenizer.save_model('/content/tokenizer_model.pkl')\n",
        "          #self.vocab_size = tokenizer.get_vocab_size()\n",
        "      elif datatype == 'test' and max_seq_len != None:\n",
        "          # try:\n",
        "          tokenizer.load_model(model_path)\n",
        "          sequences = [tokenizer.encode(sentence) for sentence in data]\n",
        "          sequences = pad_sequences(sequences, maxlen=max_seq_len, value = 0, padding='post')\n",
        "          # except:\n",
        "          # print(\"please check if tokenizer model is passed correctly!\")\n",
        "\n",
        "      return sequences, max_seq_len, tokenizer\n",
        "\n",
        "  def preprocess_data(self):\n",
        "    #label names are 'id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'\n",
        "    train_data_path = self.folder_path + '/train.txt'\n",
        "    test_data_path = self.folder_path + '/test.txt'\n",
        "    dev_data_path = self.folder_path + '/dev.txt'\n",
        "\n",
        "    with open(train_data_path, 'r', encoding='utf-8') as file:\n",
        "        train_tweets = file.readlines()\n",
        "    self.train_data = pd.DataFrame([tweet.strip().split('\\t') for tweet in train_tweets], columns=['id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'])\n",
        "    with open(test_data_path, 'r', encoding='utf-8') as file:\n",
        "        test_tweets = file.readlines()\n",
        "    self.test_data = pd.DataFrame([tweet.strip().split('\\t') for tweet in test_tweets], columns=['id', 'text'])\n",
        "    with open(dev_data_path, 'r', encoding='utf-8') as file:\n",
        "        dev_tweets = file.readlines()\n",
        "    self.dev_data = pd.DataFrame([tweet.strip().split('\\t') for tweet in dev_tweets], columns=['id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'])\n",
        "\n",
        "\n",
        "\n",
        "    self.X_train = self.train_data.apply(lambda x: clean_tweet(x['text'], \"ml\"), axis=1)\n",
        "    self.y_train = self.train_data.apply(lambda x: x[self.label_name], axis=1)\n",
        "    #print(self.X_train)\n",
        "    #print(self.y_train)\n",
        "    self.X_test = self.test_data.apply(lambda x: clean_tweet(x['text'], \"ml\"), axis=1)\n",
        "    #self.y_test = self.test_data.apply(lambda x: x[self.label_name], axis=1)\n",
        "    self.X_dev = self.dev_data.apply(lambda x: clean_tweet(x['text'], \"ml\"), axis=1)\n",
        "    self.y_dev = self.dev_data.apply(lambda x: x[self.label_name], axis=1)\n",
        "\n",
        "    self.X_train, max_seq_length, _ = self.tokenize_and_pad_tweets(self.X_train, 'train', 1000)\n",
        "    self.max_seq_length = max_seq_length\n",
        "    #print(self.max_seq_length)\n",
        "    #print(self.X_train)\n",
        "    #print(self.X_test)\n",
        "    self.X_test, _, _ = self.tokenize_and_pad_tweets(self.X_test, 'test', 1000, self.max_seq_length)\n",
        "    self.X_dev, _, _ = self.tokenize_and_pad_tweets(self.X_dev, 'test', 1000, self.max_seq_length)\n",
        "\n",
        "    # Convert labels to numerical values\n",
        "    lb = LabelBinarizer()\n",
        "    self.y_train = lb.fit_transform(self.y_train)\n",
        "    #print(self.y_train)\n",
        "    #self.y_test = lb.transform(self.y_test)\n",
        "    self.y_dev = lb.transform(self.y_dev)\n",
        "    # print(self.y_train)\n",
        "    # print(self.y_dev)\n",
        "\n",
        "    #print(self.X_train.shape)\n",
        "    #print(self.y_  train.shape)\n",
        "    #print(self.dev_data)\n",
        "    #print(self.X_dev)\n",
        "    #print(self.y_dev)\n",
        "\n",
        "  def train(self, model_type):\n",
        "    if model_type == 'SVM':\n",
        "        self.model = SVC()\n",
        "    elif model_type == 'NaiveBayes':\n",
        "        self.model = MultinomialNB()\n",
        "    elif model_type == 'RandomForest':\n",
        "        self.model = RandomForestClassifier()\n",
        "    elif model_type == 'LSTM':\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Embedding(input_dim=self.vocab_size, output_dim=150, input_length=self.max_seq_length))#ML embedding pipeline\n",
        "        self.model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "        self.model.add(Dense(1, activation='sigmoid'))\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    else:\n",
        "        print('Invalid model type')\n",
        "        return\n",
        "\n",
        "    self.model.fit(self.X_train, self.y_train, validation_data=(self.X_dev, self.y_dev), epochs=10, batch_size=16)\n",
        "\n",
        "  def test(self):\n",
        "    predictions = self.model.predict(self.X_test)\n",
        "    #print('Accuracy:', accuracy_score(self.y_test, predictions))\n",
        "    #print('F1 Score:', f1_score(self.y_test, predictions))\n",
        "\n",
        "  def run(self, model_type):\n",
        "    self.preprocess_data()\n",
        "    self.train(model_type)\n",
        "    #self.test()\n",
        "\n"
      ],
      "metadata": {
        "id": "2A0u4a95lLsj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the Folder that contains Data\n",
        "folder_path = \"/content/drive/MyDrive/hate-speech/Data\"\n",
        "#label names are 'id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'\n",
        "label_name = 'subtask_a'\n",
        "classifier = TextClassifier(folder_path,label_name)\n",
        "classifier.run('LSTM')  # Replace with the desired model type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNe4Ry2Sc0tT",
        "outputId": "b46d9086-3437-47eb-c6a5-7f1410b96be3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training WordTokenizer ...\n",
            "Saving as pickle file ...\n",
            "Loading as pickle file ...\n",
            "Loading as pickle file ...\n",
            "Epoch 1/10\n",
            "556/556 [==============================] - 109s 186ms/step - loss: 0.6547 - accuracy: 0.6431 - val_loss: 0.6257 - val_accuracy: 0.6819\n",
            "Epoch 2/10\n",
            "556/556 [==============================] - 103s 185ms/step - loss: 0.6527 - accuracy: 0.6431 - val_loss: 0.6390 - val_accuracy: 0.6819\n",
            "Epoch 3/10\n",
            "556/556 [==============================] - 104s 186ms/step - loss: 0.6528 - accuracy: 0.6431 - val_loss: 0.6295 - val_accuracy: 0.6819\n",
            "Epoch 4/10\n",
            "556/556 [==============================] - 103s 186ms/step - loss: 0.6544 - accuracy: 0.6428 - val_loss: 0.6325 - val_accuracy: 0.6819\n",
            "Epoch 5/10\n",
            "556/556 [==============================] - 101s 182ms/step - loss: 0.6525 - accuracy: 0.6431 - val_loss: 0.6308 - val_accuracy: 0.6819\n",
            "Epoch 6/10\n",
            "556/556 [==============================] - 101s 182ms/step - loss: 0.6525 - accuracy: 0.6431 - val_loss: 0.6278 - val_accuracy: 0.6819\n",
            "Epoch 7/10\n",
            "556/556 [==============================] - 103s 186ms/step - loss: 0.6519 - accuracy: 0.6431 - val_loss: 0.6263 - val_accuracy: 0.6819\n",
            "Epoch 8/10\n",
            "556/556 [==============================] - 104s 186ms/step - loss: 0.6529 - accuracy: 0.6452 - val_loss: 0.6236 - val_accuracy: 0.6819\n",
            "Epoch 9/10\n",
            "556/556 [==============================] - 102s 183ms/step - loss: 0.6511 - accuracy: 0.6388 - val_loss: 0.6269 - val_accuracy: 0.6819\n",
            "Epoch 10/10\n",
            "556/556 [==============================] - 101s 182ms/step - loss: 0.6530 - accuracy: 0.6431 - val_loss: 0.6327 - val_accuracy: 0.6819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1iU35fbHb0-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 [3.10]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}