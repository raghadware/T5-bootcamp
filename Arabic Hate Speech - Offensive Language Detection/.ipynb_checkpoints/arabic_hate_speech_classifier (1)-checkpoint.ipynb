{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Hate Speech Classification \n",
    "## Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByLDGpt1hyLM",
    "outputId": "b9e547a6-77e9-40d9-d890-71e1aecacf4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (2.9.0)\n",
      "Requirement already satisfied: pyarabic in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from pyarabic) (1.16.0)\n",
      "Requirement already satisfied: tkseem in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (0.0.3)\n",
      "Requirement already satisfied: black in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from tkseem) (22.6.0)\n",
      "Requirement already satisfied: numpy in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from tkseem) (1.26.2)\n",
      "Requirement already satisfied: farasapy in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from tkseem) (0.0.14)\n",
      "Requirement already satisfied: tqdm in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from tkseem) (4.64.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from tkseem) (0.1.99)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from black->tkseem) (0.4.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from black->tkseem) (2.5.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from black->tkseem) (4.3.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from black->tkseem) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from black->tkseem) (8.1.7)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from black->tkseem) (0.9.0)\n",
      "Requirement already satisfied: requests in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from farasapy->tkseem) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from requests->farasapy->tkseem) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from requests->farasapy->tkseem) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from requests->farasapy->tkseem) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rare/opt/anaconda3/lib/python3.9/site-packages (from requests->farasapy->tkseem) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rare/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install pyarabic\n",
    "!pip install tkseem\n",
    "#!pip install Data_Fetching\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZFGxWmNdlicW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 20:00:55.498768: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "from pyarabic.araby import normalize_ligature\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from gensim.models import FastText\n",
    "import tkseem as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Download Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-07 19:08:32--  https://alt.qcri.org/resources/OSACT2022/OSACT2022-sharedTask-train.txt\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.231\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-train.txt [following]\n",
      "--2023-12-07 19:08:32--  http://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-train.txt\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-train.txt [following]\n",
      "--2023-12-07 19:08:32--  https://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-train.txt\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1502506 (1.4M) [text/plain]\n",
      "Saving to: 'OSACT2022-sharedTask-train.txt'\n",
      "\n",
      "OSACT2022-sharedTas 100%[===================>]   1.43M  2.04MB/s    in 0.7s    \n",
      "\n",
      "2023-12-07 19:08:33 (2.04 MB/s) - 'OSACT2022-sharedTask-train.txt' saved [1502506/1502506]\n",
      "\n",
      "--2023-12-07 19:08:33--  https://alt.qcri.org/resources/OSACT2022/OSACT2022-sharedTask-dev.txt\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.231\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-dev.txt [following]\n",
      "--2023-12-07 19:08:33--  http://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-dev.txt\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-dev.txt [following]\n",
      "--2023-12-07 19:08:33--  https://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-dev.txt\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 211355 (206K) [text/plain]\n",
      "Saving to: 'OSACT2022-sharedTask-dev.txt'\n",
      "\n",
      "OSACT2022-sharedTas 100%[===================>] 206.40K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-12-07 19:08:33 (1.57 MB/s) - 'OSACT2022-sharedTask-dev.txt' saved [211355/211355]\n",
      "\n",
      "--2023-12-07 19:08:33--  https://alt.qcri.org/resources/OSACT2022/OSACT2022-sharedTask-test-tweets.txt\n",
      "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.231\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-test-tweets.txt [following]\n",
      "--2023-12-07 19:08:34--  http://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-test-tweets.txt\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-test-tweets.txt [following]\n",
      "--2023-12-07 19:08:34--  https://alt.qcri.org/resources1/OSACT2022/OSACT2022-sharedTask-test-tweets.txt\n",
      "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 348638 (340K) [text/plain]\n",
      "Saving to: 'OSACT2022-sharedTask-test-tweets.txt'\n",
      "\n",
      "OSACT2022-sharedTas 100%[===================>] 340.47K   346KB/s    in 1.0s    \n",
      "\n",
      "2023-12-07 19:08:35 (346 KB/s) - 'OSACT2022-sharedTask-test-tweets.txt' saved [348638/348638]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://alt.qcri.org/resources/OSACT2022/OSACT2022-sharedTask-train.txt\n",
    "!wget  https://alt.qcri.org/resources/OSACT2022/OSACT2022-sharedTask-dev.txt\n",
    "!wget https://alt.qcri.org/resources/OSACT2022/OSACT2022-sharedTask-test-tweets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save downloaded content to separate text files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iAbQJbw1YPD8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: /OSACT2022-sharedTask-dev.txt: No such file or directory\n",
      "cp: /OSACT2022-sharedTask-test-tweets.txt: No such file or directory\n",
      "cp: /OSACT2022-sharedTask-train.txt: No such file or directory\n",
      "mv: rename train.txt to ./Data/train.txt: No such file or directory\n",
      "mv: rename test.txt to ./Data/test.txt: No such file or directory\n",
      "mv: rename dev.txt to ./Data/dev.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp /OSACT2022-sharedTask-dev.txt /content\n",
    "!cp /OSACT2022-sharedTask-test-tweets.txt /content\n",
    "!cp /OSACT2022-sharedTask-train.txt /content\n",
    "!mkdir ./Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move all text files to `Data` folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv train.txt ./Data\n",
    "!mv test.txt ./Data\n",
    "!mv dev.txt ./Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download arabic stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rare/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words(\"arabic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**functions to clean each tweet iterating over each word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74WXSjaoi8jo",
    "outputId": "1e457bf6-dfc1-4459-a66c-4110f089f073"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rare/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#functions to clean the tweets\n",
    "def tokenize_tweet(tweet):\n",
    "    # create a TweetTokenizer object\n",
    "    tknzr = TweetTokenizer()\n",
    "    # tokenize the tweet\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    return tokens\n",
    "\n",
    "def remove_extra_spaces(words):\n",
    "    \"\"\"Removes extra whitespaces at the beginning and at the end of each word in a list\"\"\"\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        cleaned_word = ' '.join(word.split()).strip()\n",
    "        cleaned_words.append(cleaned_word)\n",
    "    return cleaned_words\n",
    "\n",
    "def remove_urls(lst):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return [re.sub(pattern, '', item).strip() for item in lst if re.sub(pattern, '', item).strip() != '']\n",
    "\n",
    "def remove_user_mentions(words):\n",
    "    \"\"\"Removes user mentions (@user) from a list of words\"\"\"\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if not word.startswith('@'):\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words\n",
    "\n",
    "def remove_punctuation(lst):\n",
    "    \"\"\"Removes punctuation from a list of strings, including single punctuation characters\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation+'؟')\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        # Remove all punctuation characters\n",
    "        item = item.translate(translator)\n",
    "        # Remove any remaining single punctuation characters\n",
    "        if item != '':\n",
    "          result.append(item)\n",
    "    return result\n",
    "\n",
    "def remove_numbers(lst):\n",
    "    \"\"\"Removes numbers from a list of strings\"\"\"\n",
    "    pattern = re.compile(r'\\d+')\n",
    "    return [re.sub(pattern, '', item) for item in lst if re.sub(pattern, '', item).strip() != '']\n",
    "\n",
    "def remove_emojis(words):\n",
    "    \"\"\"Removes emojis from a list of words\"\"\"\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        cleaned_word = ''.join(c for c in word if c not in emoji.EMOJI_DATA)\n",
    "        if cleaned_word != '':\n",
    "            cleaned_words.append(cleaned_word)\n",
    "    return cleaned_words\n",
    "\n",
    "def remove_foreign_language(lst):\n",
    "    pattern = re.compile(r'[^\\u0600-\\u06ff]+')\n",
    "    return [re.sub(pattern, \"\", item) for item in lst if re.sub(pattern, \"\", item) != '']\n",
    "\n",
    "def remove_tashkeel(lst):\n",
    "    return [normalize_ligature(strip_tashkeel(word)) for word in lst]\n",
    "\n",
    "def remove_repeated_chars(lst):\n",
    "    pattern = re.compile(r\"(\\w)\\1{2,}\")\n",
    "    return [re.sub(pattern, r\"\\1\\1\", item).strip() for item in lst if re.sub(pattern, '', item).strip() != '']\n",
    "\n",
    "def remove_stop_words(lst):\n",
    "\n",
    "    result = []\n",
    "    for word in lst:\n",
    "        if word not in STOP_WORDS:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**function to clean tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean a single tweet\n",
    "def clean_tweet(tweet,mode=\"ml\"):\n",
    "    if mode==\"ml\":\n",
    "        #tokenize tweet\n",
    "        words = tokenize_tweet(tweet)\n",
    "        #remove extra white-spaces\n",
    "        words = remove_extra_spaces(words)\n",
    "        #remove urls\n",
    "        words = remove_urls(words)\n",
    "        #remove user mentions\n",
    "        words = remove_user_mentions(words)\n",
    "        #remove punctiation\n",
    "        words = remove_punctuation(words)\n",
    "        #remove numbers\n",
    "        words = remove_numbers(words)\n",
    "        #remove emojis\n",
    "        words = remove_emojis(words)\n",
    "        #remove non-arabic charachters\n",
    "        words = remove_foreign_language(words)\n",
    "        #remove tashkeel\n",
    "        words = remove_tashkeel(words)\n",
    "        #remove repeated charachters\n",
    "        words = remove_repeated_chars(words)\n",
    "        #remove stop words\n",
    "        words = remove_stop_words(words)\n",
    "        #form a new sentence\n",
    "        sentence = form_sentence(words)\n",
    "    else:\n",
    "        words = tokenize_tweet(tweet)\n",
    "        #remove extra white-spaces\n",
    "        words = remove_extra_spaces(words)\n",
    "        #remove urls\n",
    "        words = remove_urls(words)\n",
    "        #remove user mentions\n",
    "        words = remove_user_mentions(words)\n",
    "        #remove punctiation\n",
    "        words = remove_punctuation(words)\n",
    "        #remove numbers\n",
    "        words = remove_numbers(words)\n",
    "        #remove emojis\n",
    "        words = remove_emojis(words)\n",
    "        #remove non-arabic charachters\n",
    "        words = remove_foreign_language(words)\n",
    "        #form a new sentence\n",
    "        sentence = form_sentence(words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After cleaning each word in the tweet used this function to reform the words into a sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to reform the sentence from list of words\n",
    "def form_sentence(words):\n",
    "    sentence = ' '.join(words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "iS4b7eoD6OVy"
   },
   "outputs": [],
   "source": [
    "# Reshape the input data\n",
    "    # self.X_train = self.X_train.apply(lambda x: np.reshape(x, (1, len(x), len(x[0]))))\n",
    "    # self.X_test = self.X_test.apply(lambda x: np.reshape(x, (1, len(x), len(x[0]))))\n",
    "    # self.X_dev = self.X_dev.apply(lambda x: np.reshape(x, (1, len(x), len(x[0]))))\n",
    "\n",
    "    # Convert words to vectors\n",
    "    # self.X_train = self.X_train.apply(lambda x: np.mean([self.model_w2v.wv[word] for word in x], axis=0))\n",
    "    # self.X_test = self.X_test.apply(lambda x: np.mean([self.model_w2v.wv[word] for word in x], axis=0))\n",
    "    # self.X_dev = self.X_dev.apply(lambda x: np.mean([self.model_w2v.wv[word] for word in x], axis=0))\n",
    "\n",
    "    # Reshape the input data\n",
    "    # self.X_train = self.X_train.apply(lambda x: np.reshape(x, (1, len(x), len(x[0]))))\n",
    "    # self.X_test = self.X_test.apply(lambda x: np.reshape(x, (1, len(x), len(x[0]))))\n",
    "    # self.X_dev = self.X_dev.apply(lambda x: np.reshape(x, (1, len(x), len(x[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization and padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aG9ns3Wy82he"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad_tweets(data,datatype= 'train',max_words=None,max_seq_len=None,model_path= '/tokenizer_model.pkl'):\n",
    "    \"\"\"Tokenizes tweets and pads the sequences to the length of the longest sequence in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df_column (pandas.Series): A DataFrame column containing tweets.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            numpy.ndarray: An array of padded sequences.\n",
    "            int: The vocabulary size.\n",
    "            int: The maximum sequence length.\n",
    "            Tokenizer: The tokenizer object used for the tokenization.\n",
    "    \"\"\"\n",
    "    PROJECT_PATH = os.path.realpath(os.path.dirname(__file__))\n",
    "    model_path = os.path.join(PROJECT_PATH, model_path)\n",
    "\n",
    "    if max_words == None:\n",
    "          tokenizer = tk.WordTokenizer()\n",
    "    else:\n",
    "          tokenizer = tk.WordTokenizer(vocab_size=max_words)\n",
    "\n",
    "    if datatype == 'train':\n",
    "        # Create tokenizer\n",
    "        path = os.path.join(PROJECT_PATH, './Data/tokenizer.txt')\n",
    "        df = pd.DataFrame(data,columns=['tweet'])\n",
    "        df.to_csv(path, sep='\\n', header=False,index=False)\n",
    "\n",
    "        tokenizer.train(path)\n",
    "\n",
    "        sequences = [tokenizer.encode(sentence) for sentence in data]\n",
    "        max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        sequences = pad_sequences(sequences, maxlen=max_seq_len,value = 0, padding='post')\n",
    "\n",
    "\n",
    "        tokenizer.save_model(os.path.join(PROJECT_PATH,'./Models/tokenizer_model.pkl'))\n",
    "\n",
    "    elif datatype == 'test' and max_seq_len != None:\n",
    "        try:\n",
    "            tokenizer.load_model(model_path)\n",
    "            sequences = [tokenizer.encode(sentence) for sentence in data]\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            sequences = pad_sequences(sequences, maxlen=max_seq_len,value = 0, padding='post')\n",
    "\n",
    "        except:\n",
    "            print(\"please check if tokenizer model is passed correctly!\")\n",
    "\n",
    "    return sequences, vocab_size, max_seq_len, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2A0u4a95lLsj"
   },
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "  def __init__(self, folder_path,label_name):\n",
    "    self.folder_path = folder_path\n",
    "    self.X_train, self.X_test, self.y_train, self.y_test, self.X_dev, self.y_dev = None, None, None, None, None, None\n",
    "    self.model = None\n",
    "    self.label_name = label_name\n",
    "    self.max_seq_length  = None\n",
    "\n",
    "  def tokenize_and_pad_tweets(self, data, datatype= 'train', max_words=None, max_seq_len=None, model_path= '/tokenizer_model.pkl'):\n",
    "\n",
    "      sequences = [] # Initialize sequences to an empty list\n",
    "\n",
    "      if max_words == None:\n",
    "          tokenizer = tk.WordTokenizer()\n",
    "      else:\n",
    "          tokenizer = tk.WordTokenizer(vocab_size=max_words)\n",
    "\n",
    "      if datatype == 'train':\n",
    "          path = './tokenizer.txt'\n",
    "          df = pd.DataFrame(data,columns=['tweet'])\n",
    "          df.to_csv(path, sep='\\n', header=False,index=False)\n",
    "          tokenizer.train(path)\n",
    "          sequences = [tokenizer.encode(sentence) for sentence in data]\n",
    "          max_seq_len = max(len(seq) for seq in sequences)\n",
    "          sequences = pad_sequences(sequences, maxlen=max_seq_len, value = 0, padding='post')\n",
    "          tokenizer.save_model('/tokenizer_model.pkl')\n",
    "      elif datatype == 'test' and max_seq_len != None:\n",
    "          try:\n",
    "              tokenizer.load_model(model_path)\n",
    "              sequences = [tokenizer.texts_to_sequences([sentence]) for sentence in data]\n",
    "              sequences = pad_sequences(sequences, maxlen=max_seq_len, value = 0, padding='post')\n",
    "          except:\n",
    "              print(\"please check if tokenizer model is passed correctly!\")\n",
    "\n",
    "      return sequences, max_seq_len, tokenizer\n",
    "\n",
    "  def preprocess_data(self):\n",
    "    #label names are 'id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'\n",
    "    train_data_path = self.folder_path + '/train.txt'\n",
    "    test_data_path = self.folder_path + '/test.txt'\n",
    "    dev_data_path = self.folder_path + '/dev.txt'\n",
    "    with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "        train_tweets = file.readlines()\n",
    "    self.train_data = pd.DataFrame([tweet.strip().split('\\t') for tweet in train_tweets], columns=['id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'])\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as file:\n",
    "        test_tweets = file.readlines()\n",
    "    self.test_data = pd.DataFrame([tweet.strip().split('\\t') for tweet in test_tweets], columns=['id', 'text'])\n",
    "    with open(dev_data_path, 'r', encoding='utf-8') as file:\n",
    "        dev_tweets = file.readlines()\n",
    "    self.dev_data = pd.DataFrame([tweet.strip().split('\\t') for tweet in dev_tweets], columns=['id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'])\n",
    "\n",
    "    self.X_train = self.train_data.apply(lambda x: clean_tweet(x['text'], \"ml\"), axis=1)\n",
    "    self.y_train = self.train_data.apply(lambda x: x[self.label_name], axis=1)\n",
    "    self.X_test = self.test_data.apply(lambda x: clean_tweet(x['text'], \"ml\"), axis=1)\n",
    "    #self.y_test = self.test_data.apply(lambda x: x[self.label_name], axis=1)\n",
    "    self.X_dev = self.dev_data.apply(lambda x: clean_tweet(x['text'], \"ml\"), axis=1)\n",
    "    self.y_dev = self.dev_data.apply(lambda x: x[self.label_name], axis=1)\n",
    "\n",
    "    self.X_train, max_seq_length, _ = self.tokenize_and_pad_tweets(self.X_train)\n",
    "    self.max_seq_length = max_seq_length\n",
    "    print(self.X_train)\n",
    "    self.X_test, _, _ = self.tokenize_and_pad_tweets(self.X_test, datatype='test')\n",
    "    self.X_dev, _, _ = self.tokenize_and_pad_tweets(self.X_dev, datatype='test' )\n",
    "\n",
    "    # Tokenize the text\n",
    "    # self.X_train = self.X_train.apply(word_tokenize)\n",
    "    # self.X_test = self.X_test.apply(word_tokenize)\n",
    "    # self.X_dev = self.X_dev.apply(word_tokenize)\n",
    "\n",
    "\n",
    "    ## Build the vocabulary\n",
    "    # self.model_w2v = FastText(vector_size=150, window=10, min_count=2, workers=10)\n",
    "    # self.model_w2v.build_vocab_from_freq(self.X_train.explode().value_counts())\n",
    "    ## Convert words to vectors\n",
    "    #Train Word2Vec model\n",
    "    # self.model_w2v.train(self.X_train, total_examples=len(self.X_train), epochs=10)\n",
    "    # self.X_train = self.X_train.apply(lambda x: [self.model_w2v.wv.get_vector(word) if word in self.model_w2v.wv else np.zeros(150) for word in x])\n",
    "    # self.X_test = self.X_test.apply(lambda x: [self.model_w2v.wv.get_vector(word) if word in self.model_w2v.wv else np.zeros(150) for word in x])\n",
    "\n",
    "    # self.X_dev = self.X_dev.apply(lambda x: [self.model_w2v.wv.get_vector(word) if word in self.model_w2v.wv else np.zeros(150) for word in x])\n",
    "\n",
    "\n",
    "    # Pad sequences to a fixed length\n",
    "    # max_seq_length = 50  # You can adjust this based on your data\n",
    "    # self.X_train = pad_sequences(self.X_train, maxlen=max_seq_length, dtype='float32', padding='post', truncating='post')\n",
    "    # self.X_test = pad_sequences(self.X_test, maxlen=max_seq_length, dtype='float32', padding='post', truncating='post')\n",
    "    # self.X_dev = pad_sequences(self.X_dev, maxlen=max_seq_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "    # Convert labels to numerical values\n",
    "    lb = LabelBinarizer()\n",
    "    self.y_train = lb.fit_transform(self.y_train)\n",
    "    #self.y_test = lb.transform(self.y_test)\n",
    "    self.y_dev = lb.transform(self.y_dev)\n",
    "    # print(self.y_train)\n",
    "    # print(self.y_dev)\n",
    "\n",
    "  def train(self, model_type):\n",
    "    if model_type == 'SVM':\n",
    "        self.model = SVC()\n",
    "    elif model_type == 'NaiveBayes':\n",
    "        self.model = MultinomialNB()\n",
    "    elif model_type == 'RandomForest':\n",
    "        self.model = RandomForestClassifier()\n",
    "    elif model_type == 'LSTM':\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=72, output_dim=150, input_length=self.max_seq_length))\n",
    "        self.model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    else:\n",
    "        print('Invalid model type')\n",
    "        return\n",
    "\n",
    "    self.model.fit(self.X_train, self.y_train, validation_data=(self.X_dev, self.y_dev), epochs=10, batch_size=32)\n",
    "\n",
    "  def test(self):\n",
    "    predictions = self.model.predict(self.X_test)\n",
    "    print('Accuracy:', accuracy_score(self.y_test, predictions))\n",
    "    print('F1 Score:', f1_score(self.y_test, predictions))\n",
    "\n",
    "  def run(self, model_type):\n",
    "    self.preprocess_data()\n",
    "    self.train(model_type)\n",
    "    self.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YNe4Ry2Sc0tT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/tokenizer_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g0/mmfhfgtj0hg5j2sjq28n4rvc0000gn/T/ipykernel_5430/895924701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'subtask_a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with the desired model type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/g0/mmfhfgtj0hg5j2sjq28n4rvc0000gn/T/ipykernel_5430/480350966.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, model_type)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g0/mmfhfgtj0hg5j2sjq28n4rvc0000gn/T/ipykernel_5430/480350966.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_and_pad_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g0/mmfhfgtj0hg5j2sjq28n4rvc0000gn/T/ipykernel_5430/480350966.py\u001b[0m in \u001b[0;36mtokenize_and_pad_tweets\u001b[0;34m(self, data, datatype, max_words, max_seq_len, model_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tokenizer_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mdatatype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tkseem/_base.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[1;32m    393\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{file_path}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving as pickle file ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tokenizer_model.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Add the Folder that contains Data\n",
    "folder_path = \"Data\"\n",
    "#label names are 'id', 'text', 'subtask_a', 'subtask_b', 'subtask_c1', 'subtask_c2'\n",
    "label_name = 'subtask_a'\n",
    "classifier = TextClassifier(folder_path,label_name)\n",
    "classifier.run('LSTM')  # Replace with the desired model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xnzaJD8KqSi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
